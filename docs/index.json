[{"content":"Hexadecimal (Hex) is a base-16 system used to represent binary data. Its key relationships:\n1 hex digit = 4 bits (also called a nibble) 2 hex digits = 1 byte (8 bits) In computer systems:\n1 bit = smallest binary unit, can be 0 or 1 1 byte = 8 bits 1 nibble = 4 bits 1 word = usually 4 bytes (32 bits) 1 double word = usually 8 bytes (64 bits) Example: Hex vs Binary Table Hex Binary Size A 1010 4 bits (1 nibble) 3F 0011 1111 8 bits (1 byte) 7E2A 0111 1110 0010 1010 16 bits (2 bytes) FF AA 01 1111 1111 1010 1010 0000 0001 24 bits (3 bytes) Summary 1 hex digit = 4 bits 2 hex digits = 1 byte (8 bits) 8 hex digits = 4 bytes = 1 word = 32 bits Offset Bits and Block Size The number of offset bits depends on the block size, using this formula:\n[ \\text{Offset Bits} = \\log_2(\\text{Block Size}) ]\nFor example, if the block size is 8 bytes, then:\n[ \\log_2(8) = 3 \\text{ bits} ]\nSo 3 offset bits are needed to identify the exact byte inside the block:\nOffset Bits Meaning 000 Byte 0 001 Byte 1 010 Byte 2 \u0026hellip; \u0026hellip; 111 Byte 7 Note This concept is critical when dealing with cache addressing and memory hierarchy. Understanding how offset bits map into memory blocks helps later in cache tag/index/offset separation.\n","permalink":"https://dev-helia.github.io/Tech-Blog-Website/posts/cs/principles_of_computer_composition/01_bit/","summary":"\u003cp\u003eHexadecimal (Hex) is a base-16 system used to represent binary data. Its key relationships:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e1 hex digit = 4 bits (also called a \u003cstrong\u003enibble\u003c/strong\u003e)\u003c/li\u003e\n\u003cli\u003e2 hex digits = 1 byte (8 bits)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn computer systems:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e1 bit\u003c/code\u003e = smallest binary unit, can be 0 or 1\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e1 byte\u003c/code\u003e = 8 bits\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e1 nibble\u003c/code\u003e = 4 bits\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e1 word\u003c/code\u003e = usually 4 bytes (32 bits)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e1 double word\u003c/code\u003e = usually 8 bytes (64 bits)\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"example-hex-vs-binary-table\"\u003eExample: Hex vs Binary Table\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eHex\u003c/th\u003e\n          \u003cth\u003eBinary\u003c/th\u003e\n          \u003cth\u003eSize\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eA\u003c/td\u003e\n          \u003ctd\u003e1010\u003c/td\u003e\n          \u003ctd\u003e4 bits (1 nibble)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e3F\u003c/td\u003e\n          \u003ctd\u003e0011 1111\u003c/td\u003e\n          \u003ctd\u003e8 bits (1 byte)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e7E2A\u003c/td\u003e\n          \u003ctd\u003e0111 1110 0010 1010\u003c/td\u003e\n          \u003ctd\u003e16 bits (2 bytes)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eFF AA 01\u003c/td\u003e\n          \u003ctd\u003e1111 1111 1010 1010 0000 0001\u003c/td\u003e\n          \u003ctd\u003e24 bits (3 bytes)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch2 id=\"summary\"\u003eSummary\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e1 hex digit = 4 bits\u003c/li\u003e\n\u003cli\u003e2 hex digits = 1 byte (8 bits)\u003c/li\u003e\n\u003cli\u003e8 hex digits = 4 bytes = 1 word = 32 bits\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch2 id=\"offset-bits-and-block-size\"\u003eOffset Bits and Block Size\u003c/h2\u003e\n\u003cp\u003eThe number of \u003cstrong\u003eoffset bits\u003c/strong\u003e depends on the \u003cstrong\u003eblock size\u003c/strong\u003e, using this formula:\u003c/p\u003e","title":"Hex \u0026 Binary Basics"},{"content":"Floating Point Encoding: Single Precision Use normalized base-2 scientific notation:\n+1.xxxxx... × 2^y IEEE 754 (32-bit float) splits into 3 fields: 1 bit: Sign (0 = positive, 1 = negative) 8 bits: Exponent (with bias) 23 bits: Significand / Mantissa Value = (–1)^Sign × 1.Significand × 2^(Exponent – Bias)\nBit Breakdown (32-bit float layout) Field Size Purpose Sign 1 bit 1 = negative, 0 = positive Exponent 8 bits Biased exponent Significand 23 bits Fractional part (assumes leading 1) Why Use Biased Notation for Exponent? If exponent were stored directly (signed int), comparison \u0026amp; sorting would be more complex.\nSo IEEE 754 uses bias notation, shifting exponent range to all positive values.\nBiased Formula: Actual exponent = Stored exponent – Bias Bias = 127 for float (32-bit) Bias = 1023 for double (64-bit) Example:\nStored exponent = 10000001₂ = 129 Actual exponent = 129 – 127 = 2 Exponent Value Table Actual Exponent Biased (Stored) Binary +127 254 1111 1110 0 127 0111 1111 –126 1 0000 0001 Special: ∞ / NaN 255 1111 1111 Negative exponents → closer to zero\nLarge biased values → ∞ or NaN\nSingle vs Double Precision Type Bit Length Significand Exponent Bias Float 32-bit 23 bits 8 bits 127 Double 64-bit 52 bits 11 bits 1023 Special Encoding Cases (IEEE 754) Case Exponent Significand Meaning Positive ∞ all 1s all 0s +∞ Negative ∞ all 1s all 0s –∞ NaN all 1s ≠ 0 Not a Number Zero all 0s all 0s +0 or –0 Denorm all 0s ≠ 0 Very small numbers (no leading 1) Note:\nSpecial encodings allow representation of edge cases like NaN, ∞, zero and subnormal numbers (denorms).\nYou’ll need this logic later in pipeline, FPU, and comparison instructions.\n","permalink":"https://dev-helia.github.io/Tech-Blog-Website/posts/cs/principles_of_computer_composition/02_float/","summary":"\u003ch2 id=\"floating-point-encoding-single-precision\"\u003eFloating Point Encoding: Single Precision\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eUse \u003cstrong\u003enormalized\u003c/strong\u003e base-2 scientific notation:\u003cbr\u003e\n\u003ccode\u003e+1.xxxxx... × 2^y\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eIEEE 754 (32-bit float) splits into \u003cstrong\u003e3 fields\u003c/strong\u003e:\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e1 bit\u003c/strong\u003e: Sign (0 = positive, 1 = negative)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e8 bits\u003c/strong\u003e: Exponent (with bias)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e23 bits\u003c/strong\u003e: Significand / Mantissa\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eValue = (–1)^Sign × 1.Significand × 2^(Exponent – Bias)\u003c/p\u003e\u003c/blockquote\u003e\n\u003chr\u003e\n\u003ch2 id=\"bit-breakdown-32-bit-float-layout\"\u003eBit Breakdown (32-bit float layout)\u003c/h2\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eField\u003c/th\u003e\n          \u003cth\u003eSize\u003c/th\u003e\n          \u003cth\u003ePurpose\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eSign\u003c/td\u003e\n          \u003ctd\u003e1 bit\u003c/td\u003e\n          \u003ctd\u003e1 = negative, 0 = positive\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eExponent\u003c/td\u003e\n          \u003ctd\u003e8 bits\u003c/td\u003e\n          \u003ctd\u003eBiased exponent\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eSignificand\u003c/td\u003e\n          \u003ctd\u003e23 bits\u003c/td\u003e\n          \u003ctd\u003eFractional part (assumes leading 1)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch2 id=\"why-use-biased-notation-for-exponent\"\u003eWhy Use Biased Notation for Exponent?\u003c/h2\u003e\n\u003cp\u003eIf exponent were stored directly (signed int), comparison \u0026amp; sorting would be more complex.\u003cbr\u003e\nSo IEEE 754 uses \u003cstrong\u003ebias notation\u003c/strong\u003e, shifting exponent range to all positive values.\u003c/p\u003e","title":"Floating Point Encoding (IEEE 754)"},{"content":"Representing Fractions (Binary Point) Key Idea: Fixed Binary Point Use a fixed binary point to separate negative and non-negative powers:\nExample: 0b xx.xxxx (6-bit format) Each position represents:\n2^1 | 2^0 | . | 2^-1 | 2^-2 | ... Example:\n10.101 = 1×2^1 + 0×2^0 + 1×2^-1 + 0×2^-2 + 1×2^-3 = 2.625\nScientific Notation Use base-10 or base-2 scientific form:\n2.625 = 2 × 10^0 + 6 × 10^-1 + 2 × 10^-2 + 5 × 10^-3\nSo in binary:\n2.625 = 10.101 = 1×2^1 + 0×2^0 + 1×2^-1 + 0×2^-2 + 1×2^-3\nWhy IEEE 754 Is So Important? All CPUs, GPUs, and AI accelerators rely on floating-point math, especially IEEE 754:\nUsed in languages: float / double in C, C++, Java, Python, Rust\u0026hellip; Widely applied in physics, graphics, AI, simulations\u0026hellip; IEEE 754 Updates 2008: Added Half-Precision (16-bit) Extended to Quad Precision (128-bit) Focus on more flexible and accurate representations IEEE 754 Design Goals (1) Precision at Scale Accuracy matters for compounding operations\nIEEE 754 supports both 32-bit and 64-bit FP Results are deterministic and portable (for AI/ML) (2) Handling Floating-Point Errors Robust error cases:\nNaN (Not a Number): like 0.0 / 0.0 Overflow: exceeds max range Underflow: value too small to represent +0 and -0: IEEE allows signed zero for special edge cases These enhance numerical stability and prevent hidden bugs!\n(3) Compatibility with Two’s Complement IEEE 754 works in harmony with integer arithmetic:\n+0 in IEEE 754 is binary 000...0, same as integer +0 Ensures compatibility across int/float transitions Helps CPUs optimize under mixed integer/float workloads\nSummary:\nIEEE 754 isn’t just a spec—it’s a global standard for reliable math, powering everything from Pixar rendering to neural nets to finance tools.\nNext up, we’ll dive into instruction sets and how these formats are loaded into registers!\n","permalink":"https://dev-helia.github.io/Tech-Blog-Website/posts/cs/principles_of_computer_composition/03_ieee754/","summary":"\u003ch2 id=\"representing-fractions-binary-point\"\u003eRepresenting Fractions (Binary Point)\u003c/h2\u003e\n\u003ch3 id=\"key-idea-fixed-binary-point\"\u003eKey Idea: Fixed Binary Point\u003c/h3\u003e\n\u003cp\u003eUse a fixed binary point to separate \u003cstrong\u003enegative\u003c/strong\u003e and \u003cstrong\u003enon-negative powers\u003c/strong\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExample: \u003ccode\u003e0b xx.xxxx\u003c/code\u003e (6-bit format)\u003c/li\u003e\n\u003cli\u003eEach position represents:\u003cbr\u003e\n\u003ccode\u003e2^1 | 2^0 | . | 2^-1 | 2^-2 | ...\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eExample:\u003cbr\u003e\n\u003ccode\u003e10.101\u003c/code\u003e = 1×2^1 + 0×2^0 + 1×2^-1 + 0×2^-2 + 1×2^-3 = \u003ccode\u003e2.625\u003c/code\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003ch3 id=\"scientific-notation\"\u003eScientific Notation\u003c/h3\u003e\n\u003cp\u003eUse base-10 or base-2 scientific form:\u003cbr\u003e\n\u003ccode\u003e2.625 = 2 × 10^0 + 6 × 10^-1 + 2 × 10^-2 + 5 × 10^-3\u003c/code\u003e\u003c/p\u003e","title":"Understanding IEEE 754 \u0026 Fraction Encoding"},{"content":"Instruction Formats Overview 1. What is an Instruction Format? In most mainstream instruction set architectures (ISAs), an instruction is parsed from right to left in execution.\nFor example, after an instruction like:\necall It simply ends execution. But most other instructions contain:\nrd: destination register rs1, rs2: source registers In RISC-V, instruction format strictly requires rs1 and rs2 to be placed at fixed bit positions in the 32-bit instruction word.\nThis avoids decoding complexity caused by variable field placement.\n2. The 6 RISC-V Instruction Formats Format Description Examples R-Format Uses three register inputs add, xor, mul (arithmetic/logical) I-Format Contains immediate value, 1 source register addi, lw, jalr, slli S-Format Store-type instructions sw, sb SB-Format Branch instructions beq, bge U-Format Upper immediate (20-bit) instructions lui, auipc UJ-Format Jump-type instructions jal 3. Bit-Level Structure of Formats All RISC-V instructions are 32 bits wide.\nThe bit fields are assigned based on the instruction type. Below is a standard breakdown:\nR-Type: [ funct7 | rs2 | rs1 | funct3 | rd | opcode ] I-Type: [ imm[11:0] | rs1 | funct3 | rd | opcode ] S-Type: [ imm[11:5] | rs2 | rs1 | funct3 | imm[4:0] | opcode ] SB-Type: [ imm[12], imm[10:5], rs2, rs1, funct3, imm[4:1], imm[11], opcode ] U-Type: [ imm[31:12] | rd | opcode ] UJ-Type: [ imm[20], imm[10:1], imm[11], imm[19:12], rd, opcode ] Each format encodes information differently based on what is needed for that instruction.\n4. Why So Many Formats? Each instruction type serves different purposes and needs different fields:\nR-Type: needs 3 registers → rs1, rs2, rd I-Type: needs only one register + immediate → rs1, imm S-Type: uses split immediate → imm[11:5], imm[4:0] SB-Type: uses branch offset as an immediate → needs bit-rearranged imm U/UJ-Types: use large constants or absolute addresses (e.g., for jump or base) Each format ensures fixed decoding paths and consistent field positioning.\nISC-V Assembly Examples 1. Example: Load Array Elements and Constants .section .data arr: .word 2, 4, 6, 8 # Define an array n: .word 9 # Define an integer variable \u0026#39;n\u0026#39; .section .text .globl _start _start: # Load the base address of arr into register a0 la a0, arr # a0 = base address of arr # Load array elements into a1–a4 lw a1, 0(a0) # a1 = arr[0] = 2 lw a2, 4(a0) # a2 = arr[1] = 4 lw a3, 8(a0) # a3 = arr[2] = 6 lw a4, 12(a0) # a4 = arr[3] = 8 # Load the value of variable n la a5, n # a5 = address of n lw a6, 0(a5) # a6 = value of n = 9 # Exit the program using ecall (syscall 10) li a7, 10 # syscall code 10 = exit ecall This demonstrates how .data and .text sections are used in RISC-V, and how addresses and values are handled using la (load address) and lw (load word).\n2. Example: Fibonacci Sequence in Assembly Below is a RISC-V assembly implementation of the Fibonacci sequence.\n# Fibonacci Sequence main: add t0, x0, x0 # t0 = 0 addi t1, x0, 1 # t1 = 1 la t3, n # t3 = address of n lw t3, 0(t3) # t3 = value of n fib: beq t3, x0, finish # if n == 0, jump to finish add t2, t0, t1 # t2 = t0 + t1 mv t0, t1 # move t1 → t0 mv t1, t2 # move t2 → t1 addi t3, t3, -1 # n = n - 1 j fib # jump to next iteration finish: addi a0, t0, 0 # prepare return value li a7, 1 # syscall code 1 = print integer ecall # print result li a7, 10 # syscall code 10 = exit ecall Notes: addi: add immediate value mv: pseudo-instruction for addi x, y, 0 beq: branch if equal j: unconditional jump (pseudoinstruction for jal x0, label) la: load address (pseudo-instruction expanded to auipc + addi) This example shows:\nRegister usage Looping Branching System calls via ecall This block clearly illustrates how a real algorithm like Fibonacci can be implemented directly in RISC-V instructions, showing you the flow of control and data step-by-step.\nShift Instructions \u0026amp; Formats 1. Shift Instructions 🧭 Instruction Format Summary Instruction Meaning sll rd, rs1, rs2 Shift Left Logical: rd = rs1 \u0026laquo; rs2 slli rd, rs1, imm Shift Left Logical with immediate srl rd, rs1, rs2 Shift Right Logical: rd = rs1 \u0026raquo; rs2 srli rd, rs1, imm Shift Right Logical with immediate sra rd, rs1, rs2 Shift Right Arithmetic (sign-preserving) srai rd, rs1, imm Shift Right Arithmetic with immediate These instructions shift the bits in rs1 and store the result in rd.\n2. R-Format Instruction Layout An R-type instruction is 32 bits wide and consists of the following fields:\nField Width (bits) Description funct7 7 Operation modifier rs2 5 Second source register rs1 5 First source register funct3 3 Operation selector rd 5 Destination register opcode 7 Operation category 31 25 24 20 19 15 14 12 11 7 6 0 [funct7][ rs2 ][ rs1 ][f3 ][ rd ][opcode] Note:\nAll fields are unsigned integers 5-bit fields represent values 0–31 (suitable for register numbers) funct3 and funct7 differentiate between similar instructions 3. I-Format Instruction Layout (for immediates) When an instruction has an immediate (like slli or srli), it uses I-type format.\nField Width (bits) Description imm[11:0] 12 Immediate value rs1 5 Source register funct3 3 Operation selector rd 5 Destination register opcode 7 Operation category 31 20 19 15 14 12 11 7 6 0 [ imm[11:0] ][ rs1 ][ f3 ][ rd ][opcode] Why I-format? 12 bits for the immediate field allow larger constants Only one source register (rs1) and one destination register (rd) Preserves simplicity for hardware decoder This section gives a compact overview of how bitwise shift operations are encoded and how R/I format structures differ.\nData Transfer Instructions 2. Data Transfer Instructions 🧭 Format: memop reg, offset(base_reg) memop = lw / sw / lb / sb / lh / sh / lbu / lhu reg = target register (for load) or source register (for store) offset = immediate value (in bytes) base_reg = register holding the base address The effective memory address is computed as:\neffective_address = base_reg + offset 3. Load Instructions Instruction Meaning lw rd, offset(rs1) Load 32-bit word from memory into rd lb rd, offset(rs1) Load 8-bit signed byte from memory into rd lbu rd, offset(rs1) Load 8-bit unsigned byte into rd lh rd, offset(rs1) Load 16-bit signed half-word lhu rd, offset(rs1) Load 16-bit unsigned half-word 4. Store Instructions Instruction Meaning sw rs2, offset(rs1) Store 32-bit word from rs2 into memory sb rs2, offset(rs1) Store lower 8 bits of rs2 into memory sh rs2, offset(rs1) Store lower 16 bits of rs2 into memory Note: All load/store addresses must be aligned if required by platform (e.g. 32-bit aligned for lw/sw)\n5. Register vs Memory Memory accesses are slower than registers Use registers (a0–a7 for arguments, a0–a1 for return values) when possible Stack memory is managed using sp (stack pointer) 6. Example (Assembly) .data var: .word 42 .text .globl _start _start: la a0, var # Load address of var lw a1, 0(a0) # Load 32-bit value from var to a1 (should be 42) li a2, 100 # Set a2 to 100 sw a2, 0(a0) # Store 100 back to var 7. Summary lw/sw: 32-bit word lh/sh: 16-bit half-word lb/sb: 8-bit byte lbu/lhu: load unsigned variant (no sign-extension) Choosing correct instruction ensures proper sign-extension and data width when transferring between memory and registers.\n3. Branching \u0026amp; Jump Instructions 1 Conditional Branches (SB-Format) Instruction Meaning beq rs1, rs2, label Branch if Equal bne rs1, rs2, label Branch if Not Equal blt rs1, rs2, label Branch if Less Than bge rs1, rs2, label Branch if Greater or Equal These instructions compare rs1 and rs2, and jump to the label if the condition is met.\nSB-Format encodes the target label using a 13-bit signed offset (from current PC). Offset is always 2-byte aligned, so encoded as offset / 2.\n2 Jump Instructions (UJ-Format \u0026amp; JAL) Instruction Purpose jal rd, label Jump and Link: set rd = PC + 4, then jump to label jalr rd, rs1, imm Jump and Link Register: PC = rs1 + imm, rd = PC + 4 The link part means saving the return address (i.e., where to come back) in rd.\nCommon pattern in function calls:\njal ra, func # Call function (store return address in ra) ... ret # Return (actually `jalr x0, ra, 0`) 3 Instruction Formats SB-Format (Branch) imm[12] | imm[10:5] | rs2 | rs1 | funct3 | imm[4:1] | imm[11] | opcode The 13-bit immediate is split across multiple fields. Must multiply by 2 to get actual byte offset. UJ-Format (jal) imm[20] | imm[10:1] | imm[11] | imm[19:12] | rd | opcode Immediate total 21 bits, signed. Also needs shift left by 1 (since all jumps are 2-byte aligned) I-Format (jalr) imm[11:0] | rs1 | funct3 | rd | opcode Same format as load instructions. Used in jalr for indirect jump (like function returns) 4 Assembly Example .globl _start _start: li t0, 0 # sum = 0 li t1, 10 # loop counter loop: add a0, a0, t0 # do something addi t0, t0, 1 blt t0, t1, loop # repeat if t0 \u0026lt; t1 li a7, 10 ecall 5 Summary beq, bne, blt, bge are for condition-based control flow jal, jalr are used for function call/return Always use jalr for function returns Understand immediate encoding and format is crucial for ISA decoding Bitwise Operation Instructions Bitwise operations are commonly used for flag manipulation, masking, and low-level data manipulation.\n1 Register-to-Register Operations Instruction Description and rd, rs1, rs2 Bitwise AND or rd, rs1, rs2 Bitwise OR xor rd, rs1, rs2 Bitwise XOR These operations take two source registers and apply bitwise logic, storing the result in rd.\n2 Register-Immediate Operations Instruction Description andi rd, rs1, imm Bitwise AND with immediate ori rd, rs1, imm Bitwise OR with immediate xori rd, rs1, imm Bitwise XOR with immediate Immediate is usually used for masking specific bits, such as extracting the lower byte.\n3 Example: Masking Lower 8 Bits li t0, 0x1234 # Load a 16-bit value andi t1, t0, 0xFF # Mask and keep only the lower 8 bits # t1 = 0x34 This is a typical bitmask operation, often used in:\nExtracting specific byte from word Checking flag bits Performing low-level I/O operations 4 Summary and, or, xor operate on two registers andi, ori, xori combine register with constant Commonly used with masks like 0xFF, 0xF0, 0x0F No sign extension: bitwise logic applies bit-by-bit Arithmetic Instructions Arithmetic operations include basic add/subtract, multiplication/division, and immediate operations.\n1 Register-based Arithmetic Instruction Description add rd, rs1, rs2 Addition sub rd, rs1, rs2 Subtraction mul rd, rs1, rs2 Multiplication div rd, rs1, rs2 Division These use two register operands and return the result in rd.\n2 Immediate Arithmetic Instruction Description addi rd, rs1, imm Add with constant value Immediate must fit in 12-bit signed integer. Larger constants need to use lui.\n3 Example: Basic Arithmetic li t1, 10 li t2, 3 add t3, t1, t2 # t3 = 10 + 3 sub t4, t1, t2 # t4 = 10 - 3 mul t5, t1, t2 # t5 = 10 * 3 div t6, t1, t2 # t6 = 10 / 3 U-Type Format: Dealing with Large Immediates Sometimes, constants \u0026gt; 12 bits are needed. I-Type only allows 12 bits.\nTo handle this, RISC-V defines U-Type (Upper Immediate) format.\n1 Instruction Format | imm[31:12] | rd | opcode | | 20 bits | 5 bits | 7 bits | 2 Supported Instructions Instruction Description lui Load Upper Immediate auipc Add Upper Immediate to PC They place a 20-bit value into the upper 20 bits of a register (imm \u0026lt;\u0026lt; 12), which can then be combined with addi.\n3 Example lui t0, 0x12345 # t0 = 0x12345000 addi t0, t0, 0x67 # t0 = 0x12345067 (final constant) This two-step process is how RISC-V constructs full 32-bit constants.\nSpecial Instruction nop: No Operation nop stands for \u0026ldquo;no operation\u0026rdquo;. When executed, it performs no effect on the program state. Implementation Detail In RISC-V, nop is a pseudo-instruction:\nnop # Equivalent to: addi x0, x0, 0 It uses register x0 (always 0), so it does nothing.\nCommon Use Cases Scenario Description Pipeline Padding Inserted to avoid hazards (especially in pipelined CPUs) Instruction Alignment Used to align instructions on memory boundaries Debugging/Breakpoints As placeholders when modifying instruction flow Delay Slots (legacy) In older architectures, sometimes used to fill delay slots nop is simple but often essential in low-level optimization and hardware debugging contexts.\n","permalink":"https://dev-helia.github.io/Tech-Blog-Website/posts/cs/principles_of_computer_composition/04_isa/","summary":"\u003ch1 id=\"instruction-formats-overview\"\u003eInstruction Formats Overview\u003c/h1\u003e\n\u003ch2 id=\"1-what-is-an-instruction-format\"\u003e1. What is an Instruction Format?\u003c/h2\u003e\n\u003cp\u003eIn most mainstream instruction set architectures (ISAs), an instruction is parsed from \u003cstrong\u003eright to left\u003c/strong\u003e in execution.\u003c/p\u003e\n\u003cp\u003eFor example, after an instruction like:\u003c/p\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003eecall\n\u003c/code\u003e\u003c/pre\u003e\u003cp\u003eIt simply ends execution. But most other instructions contain:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003erd\u003c/code\u003e: destination register\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003ers1\u003c/code\u003e, \u003ccode\u003ers2\u003c/code\u003e: source registers\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn \u003cstrong\u003eRISC-V\u003c/strong\u003e, instruction format \u003cstrong\u003estrictly requires\u003c/strong\u003e \u003ccode\u003ers1\u003c/code\u003e and \u003ccode\u003ers2\u003c/code\u003e to be placed at fixed bit positions in the 32-bit instruction word.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThis avoids decoding complexity caused by variable field placement.\u003c/p\u003e","title":"Instruction Set Architecture"},{"content":"1. How Many Registers in RISC-V? RISC-V defines 32 general-purpose registers:\nx0 ~ x31 (each 32-bit, holds a word)\nTrade-off:\nMore registers = more variables can be stored But → harder to access quickly → slower hardware 2. Register Classification Type Alias Range Usage Hint Saved regs s0~s1 x8~x9 Values preserved across function calls s2~s11 x18~x27 Temp regs t0~t2 x5~x7 Temporary variables t3~t6 x28~x31 Note: Registers have no inherent type, unlike variables in C.\nTheir role depends on how you use them.\n3. Special Register: x0 (Zero Register) x0 is hardwired to 0\nAlways contains 0 Any attempt to write → has no effect Why useful?\nUsed in constant comparisons, clearing values, etc. Saves instruction space by avoiding explicit constants Summary Total: 32 registers (x0 to x31) Key takeaway: understand naming convention + classification + behavior ","permalink":"https://dev-helia.github.io/Tech-Blog-Website/posts/cs/principles_of_computer_composition/05_registers/","summary":"\u003ch3 id=\"1-how-many-registers-in-risc-v\"\u003e1. How Many Registers in RISC-V?\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003eRISC-V defines \u003cstrong\u003e32 general-purpose registers\u003c/strong\u003e:\u003cbr\u003e\n\u003ccode\u003ex0\u003c/code\u003e ~ \u003ccode\u003ex31\u003c/code\u003e (each 32-bit, holds a \u003cstrong\u003eword\u003c/strong\u003e)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eTrade-off:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMore registers = more variables can be stored\u003c/li\u003e\n\u003cli\u003eBut → harder to access quickly → slower hardware\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr\u003e\n\u003ch3 id=\"2-register-classification\"\u003e2. Register Classification\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eType\u003c/th\u003e\n          \u003cth\u003eAlias\u003c/th\u003e\n          \u003cth\u003eRange\u003c/th\u003e\n          \u003cth\u003eUsage Hint\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eSaved regs\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003es0\u003c/code\u003e~\u003ccode\u003es1\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ex8\u003c/code\u003e~\u003ccode\u003ex9\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eValues preserved across function calls\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003es2\u003c/code\u003e~\u003ccode\u003es11\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ex18\u003c/code\u003e~\u003ccode\u003ex27\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eTemp regs\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003et0\u003c/code\u003e~\u003ccode\u003et2\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ex5\u003c/code\u003e~\u003ccode\u003ex7\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003eTemporary variables\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003et3\u003c/code\u003e~\u003ccode\u003et6\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003ccode\u003ex28\u003c/code\u003e~\u003ccode\u003ex31\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cblockquote\u003e\n\u003cp\u003eNote: Registers have \u003cstrong\u003eno inherent type\u003c/strong\u003e, unlike variables in C.\u003cbr\u003e\nTheir role depends on how you use them.\u003c/p\u003e","title":"Registers File reg[]"},{"content":"Clock Cycle Graph:\nFrom cs61c A single-cycle CPU completes all stages of an instruction within one long clock cycle.\nEach instruction goes through the following five stages:\nAll stages of an instruction completed within one long clock cycle\nClock cycle sufficiently long to allow each instruction to complete all stages without interruption 1. Instruction Fetch → 2. Decode/Register Read → 3. Execute → 4. Memory → 5. Reg. Write Instruction Fetch Decode / Register Read Execute Memory Access Register Write Back The clock cycle is long enough to complete all these stages without interruption.\nInstruction Execution Stages (RISC-V) Stage Name Description IF Instruction Fetch Fetch the instruction from IMEM, increment PC += 4 ID Instruction Decode Decode the instruction, read rs1 and rs2 EX Execute Perform computation via ALU (arithmetic or address calculation) MEM Memory Access lw: read from memory; sw: write to memory WB Write Back Write result to rd Every instruction passes through IF, ID, EX, WB.\nOnly lw and sw instructions go through the MEM stage.\nProcessor: Design Principles 5 Steps to Design a Processor:\nAnalyze instruction set → datapath requirements Select set of datapath components \u0026amp; establish clock methodology Assemble datapath meeting the requirements Analyze implementation of each instruction to determine setting of control points that affect register transfer Assemble the control logic Formulate Logic Equations Design Circuits Control logic determines how the CPU executes each instruction\nDetermining control signals Any time a datapath element has an input that changes behavior, it requires a control signal e.g., ALU operation, read/write signal Use a MUX when multiple sources decide the same destination Different instructions → different control signals Your control signals will change based on your datapath Your datapath will change based on your ISA The \u0026ldquo;Iron Law\u0026rdquo; of Processor Performance Time / Program = (Instructions / Program) × (Cycles / Instruction) × (Time / Cycle) This formula breaks down execution time into three key components:\n1. Instructions / Program How many instructions are executed in a program? Affected by: compiler, algorithm, and ISA design Examples: CISC (Complex Instruction Set Computer) Fewer instructions needed to do more work RISC (Reduced Instruction Set Computer) Simpler instructions, but more of them may be needed 2. CPI (Cycles / Instruction) How many clock cycles does each instruction take? Affected by: instruction type, pipeline structure, data hazards Examples: RISC-V aims for CPI ≈ 1 (1 instruction per cycle) Complex instructions (e.g., DIV) may take multiple cycles 3. Time / Cycle How long is each clock cycle? Affected by: technology node, clock frequency, and fabrication Examples: Time per cycle = 1 / frequency 2GHz → 0.5 nanoseconds per cycle Energy per Program Energy / Program = (Instructions / Program) × (Energy / Instruction) Alternative form:\nEnergy / Program ∝ (Instructions / Program) × C × V² Where:\nC = capacitance, depends on design and number of cores V = supply voltage Energy “Iron Law” Performance = Power × Energy Efficiency = (Joules / Second) × (Tasks / Joule) = Tasks / Second Key Implications Energy efficiency (e.g., instructions per Joule) is a critical metric For power-constrained systems (e.g., data centers): Need higher energy efficiency to deliver more performance per watt For energy-constrained systems (e.g., phones): Need better energy efficiency to extend battery life ","permalink":"https://dev-helia.github.io/Tech-Blog-Website/posts/cs/principles_of_computer_composition/07_cpu/","summary":"\u003ch2 id=\"clock-cycle\"\u003eClock Cycle\u003c/h2\u003e\n\u003cp\u003e\u003cimg alt=\"alt text\" loading=\"lazy\" src=\"/Tech-Blog-Website/posts/cs/principles_of_computer_composition/07_cpu/image.png\"\u003e\n\u003cstrong\u003eGraph:\u003c/strong\u003e\u003cbr\u003e\n\u003cem\u003eFrom cs61c\u003c/em\u003e\nA \u003cstrong\u003esingle-cycle CPU\u003c/strong\u003e completes \u003cstrong\u003eall stages\u003c/strong\u003e of an instruction within \u003cstrong\u003eone long clock cycle\u003c/strong\u003e.\u003c/p\u003e\n\u003cp\u003eEach instruction goes through the following five stages:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eAll stages of an instruction completed \u003cstrong\u003ewithin one long clock cycle\u003c/strong\u003e\u003c/p\u003e\u003c/blockquote\u003e\n\u003cul\u003e\n\u003cli\u003eClock cycle sufficiently long to allow each instruction to complete \u003cstrong\u003eall stages without interruption\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e1. Instruction Fetch \n→ 2. Decode/Register Read \n→ 3. Execute \n→ 4. Memory \n→ 5. Reg. Write\n\u003c/code\u003e\u003c/pre\u003e\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eInstruction Fetch\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eDecode / Register Read\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eExecute\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMemory Access\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eRegister Write Back\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cblockquote\u003e\n\u003cp\u003eThe clock cycle is long enough to complete all these stages without interruption.\u003c/p\u003e","title":"CPU"},{"content":"Switches The basic element of physical implementations Convention: if input is a 1, the switch is asserted A Z o——o 💡 | | === === Open switch if A is 0 (unasserted) and turn OFF light bulb (Z) Close switch if A is 1 (asserted) and turn ON light bulb (Z) Maximum Clock Frequency What is the max frequency of this circuit?\nLimited by how much time needed to get correct Next State to Register (t_setup constraint) Equation:\nMax Delay = CLK-to-Q Delay + CL Delay + Setup Time Min Period = Max Delay Max Freq = 1 / Min Period Assumes: Max Delay \u0026gt; Hold Time\nArithmetic Logic Unit (ALU) Most processors contain a special logic block called the Arithmetic Logic Unit (ALU) We\u0026rsquo;ll show you an easy one that does ADD, SUB, bitwise AND, and bitwise OR Schematic:\nA────┐ │32 B────┘ ┌──────┐ │ ALU │────── R └──────┘ │ 32 Behavior Table:\nwhen S=00, R = A + B when S=01, R = A - B when S=10, R = A AND B when S=11, R = A OR B ","permalink":"https://dev-helia.github.io/Tech-Blog-Website/posts/cs/principles_of_computer_composition/06_circuits/","summary":"\u003ch1 id=\"switches\"\u003eSwitches\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003eThe basic element of physical implementations\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eConvention\u003c/strong\u003e: if input is a \u003ccode\u003e1\u003c/code\u003e, the switch is \u003cstrong\u003easserted\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cpre tabindex=\"0\"\u003e\u003ccode\u003e   A         Z\n   o——o      💡\n    |         |\n   ===        ===\n\u003c/code\u003e\u003c/pre\u003e\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eOpen\u003c/strong\u003e switch if A is \u003ccode\u003e0\u003c/code\u003e (unasserted) and turn \u003cstrong\u003eOFF\u003c/strong\u003e light bulb (Z)\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eClose\u003c/strong\u003e switch if A is \u003ccode\u003e1\u003c/code\u003e (asserted) and turn \u003cstrong\u003eON\u003c/strong\u003e light bulb (Z)\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"maximum-clock-frequency\"\u003eMaximum Clock Frequency\u003c/h1\u003e\n\u003cp\u003eWhat is the max frequency of this circuit?\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLimited by how much time needed to get correct \u003cstrong\u003eNext State to Register\u003c/strong\u003e (\u003ccode\u003et_setup\u003c/code\u003e constraint)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003eEquation:\u003c/strong\u003e\u003c/p\u003e","title":"Circuits"},{"content":"1. Current Datapath Graph:\nFrom cs61c\n2. Component Table Abbreviation Full Name Function IMEM Instruction Memory Stores instruction codes, used for fetch Reg[] Register File Register file containing 32 registers (x0 ~ x31) Imm.Gen Immediate Generator Extracts immediate from instruction inst Instruction Current instruction fetched from IMEM rs1 Register Source 1 First input operand from Reg[] rs2 Register Source 2 Second input operand from Reg[] DMEM Data Memory Used in lw/sw instructions ImmSel Immediate Select Selects immediate type (I, S, B, U, J) BSel B Operand Select Selects between rs2 or imm as second operand ALUSel ALU Operation Select ALU operation selector (add, sub, logic, etc.) MemRW Memory Read/Write Controls memory read/write (lw or sw) DataR Data Read Data read from DMEM Addr Address Address sent to DMEM Mem Memory Memory module WB Write Back Controls whether to write back ALU or memory result to Reg[] 3. IMEM vs DMEM Storage Function Content IMEM Instruction Memory Instruction codes DMEM Data Memory Data accessed by lw/sw 4. rs1 vs rs2 vs rd Field Meaning Example (add x1, x2, x3) rs1 First source operand x2 rs2 Second source operand x3 rd Destination register x1 5. Immediate Generator (Imm.Gen) Instruction Example Extracted Immediate addi x1, x2, 10 10 sw x1, 100(x2) 100 beq x1, x2, -8 -8 6. ALUSel \u0026amp; BSel Signal Function Options ALUSel Selects ALU operation add, sub, AND, OR, XOR, shift BSel Selects second operand rs2 or imm 7. MemRW (Memory Read/Write) Signal Meaning 0 Read (lw) 1 Write (sw) 8. WB (Write Back) Signal Meaning 0 Write back ALU result 1 Write back memory data from DMEM 9. Single-Cycle RISC-V RV32I Datapath (Insert single-cycle datapath diagram here)\n10. Control Unit (Control Signals) Control Bits Diagram:\n(Insert control lines marked on datapath)\nControl Bits Explanation:\nPCSel: Does this instruction change the PC? What\u0026rsquo;s the next PC? ImmSel: Does this instruction use an immediate? What type? RegWEn: Does it write to rd? BrUn: Is the branch signed or unsigned? BSel: Does ALU use rs2 or imm? ASel: Does ALU use rs1 or PC? ALUSel: What operation does ALU perform? MemRW: Should we read/write from memory? WBSel: What result to write back to rd? 11. Key Control Signal Table Control Signal Function Example Instructions PCSel Select next PC (sequential or branch) JAL, BEQ, BNE ImmSel Select immediate type (I, S, B, U, J) ADDI, LW, SW RegWEn Write back to rd register or not ADD, ADDI, LW BrUn Unsigned branch flag BLTU, BGEU BSel Select 2nd ALU operand (rs2 or imm) ADDI (imm), ADD (rs2) ASel Select 1st ALU operand (rs1 or PC) AUIPC (uses PC) ALUSel ALU operation (add, sub, shift, etc.) ADD, SUB, AND, OR MemRW Access data memory (read = 0, write = 1) LW (read), SW (write) WBSel Select data to write back ADD → ALU, LW → Memory 12. Multicycle Stages Stage Meaning Function Description IF Instruction Fetch Fetch instruction from IMEM, PC += 4 ID Instruction Decode Decode and read rs1/rs2 EX Execute ALU calculates (arithmetic or address) MEM Memory Access LW read / SW write WB Write Back Write result to rd 13. Instruction Example (lw x1, 0(x2)) Cycle Stage Action Description 1 IF Fetch instruction Load instruction into IR 2 ID Decode x2 Read value of x2, prepare ALU input 3 EX Calc x2 + offset ALU adds x2 + 0 4 MEM Memory access Read data from DMEM at address 5 WB Write x1 Store memory data to x1 14. Control Signal Timing Table Cycle Signals Meaning IF PCWrite=1, IRWrite=1, ALUSrcA=0 PC+4, fetch next instr ALUSrcB=01, ALUOp=00, IorD=0 ID ALUSrcA=0, ALUSrcB=11, ALUOp=00 Calc x2 + offset EX ALUSrcA=1, ALUSrcB=10, ALUOp=00 Execute address calc MEM MemRead=1, IorD=1 Read from DMEM WB RegWrite=1, MemtoReg=1 Write value to rd 15. FSM (Simplified State Diagram) State 0: IF → Next: State 1 State 1: ID → Next: State 2 State 2: EX (address calculation) → Next: State 3 State 3: MEM (memory read) → Next: State 4 State 4: WB (write back) → Next: State 0 16. Key Datapath Components per Stage (Multicycle) Stage Modules Used IF PC, IMEM, ALU (PC+4), MUX (ALUSrcB) ID RegFile, Imm.Gen, ALU (offset prep) EX ALU (x2+offset), MUX (ALUSrcA/B) MEM Memory (DMEM) WB RegFile, MUX (MemtoReg) 17. Multicycle Execution Summary lw = load word, load data from memory Split into 5 cycles (IF, ID, EX, MEM, WB) One action per cycle Control signals generated per cycle by FSM Hardware reused across cycles (e.g., ALU for both PC+4 and address calc) Saves hardware, increases control complexity 18. Single-Cycle vs. Multicycle CPU Comparison Dimension Single-Cycle CPU Multicycle CPU Instruction Duration One cycle per instruction 4~6 cycles per instruction Control Type Combinational logic Finite State Machine (FSM) Datapath Usage Modules used once per instruction Modules reused across cycles Signal Control Decided once per instruction Decided separately each cycle Instruction Flow Fetch → Decode → Execute → Write Back Step-by-step execution Clock Period Longest instruction determines cycle Shorter cycles by splitting stages Resource Efficiency Low (idle after use) High (reuse modules per cycle) Teaching Suitability Simpler, good for intro Closer to real CPU implementation 19. Execution Flow: Single-Cycle PC fetch instruction Decode instruction Reg[x2] + 0 Access memory Write to x1 One cycle must complete all Cycle must match longest instruction 20. Execution Flow: Multicycle (Incremental) Cycle 1: IF → Fetch instruction from PC Cycle 2: ID → Decode, read x2 Cycle 3: EX → ALU computes address = x2 + 0 Cycle 4: MEM → Read memory Cycle 5: WB → Write to x1 [Fetch] → [Decode] → [Address Calc] → [Memory Access] → [Write Back] ","permalink":"https://dev-helia.github.io/Tech-Blog-Website/posts/cs/principles_of_computer_composition/08_datapath/","summary":"\u003ch1 id=\"1-current-datapath\"\u003e1. Current Datapath\u003c/h1\u003e\n\u003cp\u003e\u003cimg alt=\"alt text\" loading=\"lazy\" src=\"/Tech-Blog-Website/posts/cs/principles_of_computer_composition/08_datapath/image.png\"\u003e\n\u003cimg alt=\"alt text\" loading=\"lazy\" src=\"/Tech-Blog-Website/posts/cs/principles_of_computer_composition/08_datapath/image-2.png\"\u003e\n\u003cimg alt=\"alt text\" loading=\"lazy\" src=\"/Tech-Blog-Website/posts/cs/principles_of_computer_composition/08_datapath/image-1.png\"\u003e\n\u003cstrong\u003eGraph:\u003c/strong\u003e\u003cbr\u003e\n\u003cem\u003eFrom cs61c\u003c/em\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch1 id=\"2-component-table\"\u003e2. Component Table\u003c/h1\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eAbbreviation\u003c/th\u003e\n          \u003cth\u003eFull Name\u003c/th\u003e\n          \u003cth\u003eFunction\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eIMEM\u003c/td\u003e\n          \u003ctd\u003eInstruction Memory\u003c/td\u003e\n          \u003ctd\u003eStores instruction codes, used for fetch\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eReg[]\u003c/td\u003e\n          \u003ctd\u003eRegister File\u003c/td\u003e\n          \u003ctd\u003eRegister file containing 32 registers (x0 ~ x31)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eImm.Gen\u003c/td\u003e\n          \u003ctd\u003eImmediate Generator\u003c/td\u003e\n          \u003ctd\u003eExtracts immediate from instruction\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003einst\u003c/td\u003e\n          \u003ctd\u003eInstruction\u003c/td\u003e\n          \u003ctd\u003eCurrent instruction fetched from IMEM\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ers1\u003c/td\u003e\n          \u003ctd\u003eRegister Source 1\u003c/td\u003e\n          \u003ctd\u003eFirst input operand from Reg[]\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ers2\u003c/td\u003e\n          \u003ctd\u003eRegister Source 2\u003c/td\u003e\n          \u003ctd\u003eSecond input operand from Reg[]\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eDMEM\u003c/td\u003e\n          \u003ctd\u003eData Memory\u003c/td\u003e\n          \u003ctd\u003eUsed in lw/sw instructions\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eImmSel\u003c/td\u003e\n          \u003ctd\u003eImmediate Select\u003c/td\u003e\n          \u003ctd\u003eSelects immediate type (I, S, B, U, J)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eBSel\u003c/td\u003e\n          \u003ctd\u003eB Operand Select\u003c/td\u003e\n          \u003ctd\u003eSelects between rs2 or imm as second operand\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eALUSel\u003c/td\u003e\n          \u003ctd\u003eALU Operation Select\u003c/td\u003e\n          \u003ctd\u003eALU operation selector (add, sub, logic, etc.)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eMemRW\u003c/td\u003e\n          \u003ctd\u003eMemory Read/Write\u003c/td\u003e\n          \u003ctd\u003eControls memory read/write (\u003ccode\u003elw\u003c/code\u003e or \u003ccode\u003esw\u003c/code\u003e)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eDataR\u003c/td\u003e\n          \u003ctd\u003eData Read\u003c/td\u003e\n          \u003ctd\u003eData read from DMEM\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eAddr\u003c/td\u003e\n          \u003ctd\u003eAddress\u003c/td\u003e\n          \u003ctd\u003eAddress sent to DMEM\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eMem\u003c/td\u003e\n          \u003ctd\u003eMemory\u003c/td\u003e\n          \u003ctd\u003eMemory module\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWB\u003c/td\u003e\n          \u003ctd\u003eWrite Back\u003c/td\u003e\n          \u003ctd\u003eControls whether to write back ALU or memory result to Reg[]\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch1 id=\"3-imem-vs-dmem\"\u003e3. IMEM vs DMEM\u003c/h1\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eStorage\u003c/th\u003e\n          \u003cth\u003eFunction\u003c/th\u003e\n          \u003cth\u003eContent\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eIMEM\u003c/td\u003e\n          \u003ctd\u003eInstruction Memory\u003c/td\u003e\n          \u003ctd\u003eInstruction codes\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eDMEM\u003c/td\u003e\n          \u003ctd\u003eData Memory\u003c/td\u003e\n          \u003ctd\u003eData accessed by lw/sw\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch1 id=\"4-rs1-vs-rs2-vs-rd\"\u003e4. rs1 vs rs2 vs rd\u003c/h1\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eField\u003c/th\u003e\n          \u003cth\u003eMeaning\u003c/th\u003e\n          \u003cth\u003eExample (\u003ccode\u003eadd x1, x2, x3\u003c/code\u003e)\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ers1\u003c/td\u003e\n          \u003ctd\u003eFirst source operand\u003c/td\u003e\n          \u003ctd\u003ex2\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003ers2\u003c/td\u003e\n          \u003ctd\u003eSecond source operand\u003c/td\u003e\n          \u003ctd\u003ex3\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003erd\u003c/td\u003e\n          \u003ctd\u003eDestination register\u003c/td\u003e\n          \u003ctd\u003ex1\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch1 id=\"5-immediate-generator-immgen\"\u003e5. Immediate Generator (Imm.Gen)\u003c/h1\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eInstruction Example\u003c/th\u003e\n          \u003cth\u003eExtracted Immediate\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003eaddi x1, x2, 10\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e10\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003esw x1, 100(x2)\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e100\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e\u003ccode\u003ebeq x1, x2, -8\u003c/code\u003e\u003c/td\u003e\n          \u003ctd\u003e-8\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch1 id=\"6-alusel--bsel\"\u003e6. ALUSel \u0026amp; BSel\u003c/h1\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eSignal\u003c/th\u003e\n          \u003cth\u003eFunction\u003c/th\u003e\n          \u003cth\u003eOptions\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eALUSel\u003c/td\u003e\n          \u003ctd\u003eSelects ALU operation\u003c/td\u003e\n          \u003ctd\u003eadd, sub, AND, OR, XOR, shift\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eBSel\u003c/td\u003e\n          \u003ctd\u003eSelects second operand\u003c/td\u003e\n          \u003ctd\u003ers2 or imm\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch1 id=\"7-memrw-memory-readwrite\"\u003e7. MemRW (Memory Read/Write)\u003c/h1\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eSignal\u003c/th\u003e\n          \u003cth\u003eMeaning\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e0\u003c/td\u003e\n          \u003ctd\u003eRead (\u003ccode\u003elw\u003c/code\u003e)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003eWrite (\u003ccode\u003esw\u003c/code\u003e)\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch1 id=\"8-wb-write-back\"\u003e8. WB (Write Back)\u003c/h1\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eSignal\u003c/th\u003e\n          \u003cth\u003eMeaning\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e0\u003c/td\u003e\n          \u003ctd\u003eWrite back ALU result\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e1\u003c/td\u003e\n          \u003ctd\u003eWrite back memory data from DMEM\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch1 id=\"9-single-cycle-risc-v-rv32i-datapath\"\u003e9. Single-Cycle RISC-V RV32I Datapath\u003c/h1\u003e\n\u003cp\u003e\u003cem\u003e(Insert single-cycle datapath diagram here)\u003c/em\u003e\u003c/p\u003e","title":"Data Path"},{"content":"Cache Structure: Tag | Index | Offset | Data A memory address is divided into:\nTag: Identifies which block the data belongs to. Index: Points to a specific cache line (set). Offset: Locates the exact byte inside a block. Data: The actual value stored. For example, in a 4KB Direct-Mapped Cache with 64B block size:\nTag Index Offset Data Block (64B) 0x1A3 010 000000 64 bytes of data 0x2B4 101 000000 64 bytes of data Cache Lookup Process When the CPU sends a memory request:\nUse the Index to locate a cache line. Compare the stored Tag with the address tag. If matched → Hit, use Offset to extract the byte. If not matched → Miss, load data from main memory and update cache. Address Breakdown When the CPU accesses memory (e.g., a 32-bit address), the cache splits the address into:\nOffset (for block position) Index (to find the set) Tag (to verify data identity) Steps:\nCPU issues an address. Cache splits it: Tag + Index + Offset. Locate set using Index. Match Tag. On Hit → Read directly. On Miss → Fetch from memory, update cache. Write Policy When writing data, cache uses one of two strategies:\nWrite-Through Write to both Cache and Memory immediately. Simple to manage, consistent. Slower due to double writes. Write-Back Write only to Cache. Mark block as dirty. Update memory only when block is replaced. Faster, but more complex (requires dirty bit tracking). Write Miss Policies What happens if the data to be written is not in cache?\nWrite Allocate: Load into cache, then write. No Write Allocate: Write directly to memory without loading cache. Write Hit Policy Write Miss Policy Name Write-Through No Write Allocate Simple, but slower Write-Back Write Allocate Fast, used in modern CPUs What is Cache Miss? Cache Hit: When the data you want is already in cache:\nCPU: \u0026ldquo;Perfect, data is right here!\u0026rdquo;\nCache Miss: When the data is not in cache:\nCPU: \u0026ldquo;Ugh, I have to go all the way to memory to get it!\u0026rdquo;\nSources of Cache Misses: The 3Cs 1. Compulsory Miss (cold start, process switch, first access) Caused by: First-time access Unavoidable initially Effect diminishes in long-running programs 2. Capacity Miss Caused by: Cache is too small to hold all needed blocks Even full associativity won\u0026rsquo;t help if total size is insufficient 3. Conflict Miss Caused by: Multiple addresses map to the same cache index Happens when associativity is low (e.g., direct-mapped cache) Table Summary of 3Cs Miss Type Cause Optimization Compulsory Miss First-time access, never stored in cache Unavoidable, but large-block prefetch can help Capacity Miss Cache too small to hold all needed data Increase cache size or use better replacement Conflict Miss Different addresses map to same index, overwrite happens Increase associativity or optimize access Mnemonic Table (Human Interpretation) Miss Type Cause Interpretation Optimization Direction Compulsory Miss First-time access “Cold start” problem Prefetch or wait it out Capacity Miss Cache too small “Not enough space for all” Increase size or code locality Conflict Miss Multiple addresses to same index “Collision, eviction occurs” Higher associativity Mnemonic Summary: Compulsory is the first-timer,\nCapacity runs out of room,\nConflict causes a fight.\nCache Mapping Strategies 1. Direct-Mapped (DM) Each address maps to one set. Simple and fast, but high conflict rate. 2. Set-Associative (SA) Each set holds multiple blocks. Reduces conflicts, balances speed and flexibility. Common: 2-Way, 4-Way, 8-Way SA 3. Fully Associative (FA) Any block can go anywhere in cache. Maximum flexibility, no index needed. Expensive, slower. Mode Flexibility Conflict Rate Cost DM Least Flexible High Cheapest SA Balanced Medium Common FA Most Flexible Low Costly Cache Associativity The full name is: Cache Associativity\nAt its core, it answers this question:\n\u0026ldquo;How many possible locations in cache can a memory address be placed into?\u0026rdquo;\nAnalogy: You are a lazy college student. Cache is your library seat.\nYou’re going to study at a specific table. The rules for picking a seat differ depending on associativity:\nAssociativity Type Seat Selection Rule Analogy Explanation DM (Direct Mapped) You can only sit at table 7. If someone else is there, you must wait. Each memory address can only go to one location → severe conflict 2-Way SA You can choose between table 7 and table 8 Each set has 2 candidate spots → fewer conflicts FA (Fully Associative) You can sit anywhere in the room Any cache block can go anywhere → minimal conflict So, what does “increasing associativity” mean? It means:\nInstead of allowing 1 block per set, we allow 2 blocks, 4 blocks, or 8 blocks.\nIn other words:\nHigher associativity More flexibility Fewer mapping collisions Example: You have an array: arr[0], arr[64], arr[128], \u0026hellip;\nEvery 64 bytes, the address maps to the same cache index.\nIf you use DM (Direct Mapped) Cache:\nOne index can only hold one block So the later blocks will evict the earlier ones → Conflict Miss If you use a 4-Way SA (4-Way Set Associative):\nThe same set can store 4 blocks Then arr[0], arr[64], arr[128], arr[192] can all coexist Conflict Misses are greatly reduced Summary Table Associativity Analogy Conflict Rate Search Speed Cost 1-Way (DM) Fixed seat High Fast Cheapest N-Way (Set Assoc) A few seat options Medium Medium Common Fully Associative Sit anywhere Lowest Slowest Most costly Replacement Policy When a set is full, we must evict one block to insert new data.\nCommon Policies Policy Description Notes LRU Least Recently Used Most common FIFO First In First Out Simple but naive Random Random block Easy, less optimal LFU Least Frequently Used Precise but costly LRU is preferred Matches temporal locality Frequently accessed data stays in cache Implemented with timestamps or stack structures Final Summary Tag identifies the data Index locates the cache line Offset finds the byte inside the block Write and replacement policies impact performance More associativity means fewer conflict misses ","permalink":"https://dev-helia.github.io/Tech-Blog-Website/posts/cs/principles_of_computer_composition/11_cache/","summary":"\u003ch1 id=\"cache-structure-tag--index--offset--data\"\u003eCache Structure: Tag | Index | Offset | Data\u003c/h1\u003e\n\u003cp\u003eA memory address is divided into:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTag\u003c/strong\u003e: Identifies which block the data belongs to.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eIndex\u003c/strong\u003e: Points to a specific cache line (set).\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eOffset\u003c/strong\u003e: Locates the exact byte inside a block.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eData\u003c/strong\u003e: The actual value stored.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eFor example, in a \u003cstrong\u003e4KB Direct-Mapped Cache\u003c/strong\u003e with \u003cstrong\u003e64B block size\u003c/strong\u003e:\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eTag\u003c/th\u003e\n          \u003cth\u003eIndex\u003c/th\u003e\n          \u003cth\u003eOffset\u003c/th\u003e\n          \u003cth\u003eData Block (64B)\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e0x1A3\u003c/td\u003e\n          \u003ctd\u003e010\u003c/td\u003e\n          \u003ctd\u003e000000\u003c/td\u003e\n          \u003ctd\u003e64 bytes of data\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003e0x2B4\u003c/td\u003e\n          \u003ctd\u003e101\u003c/td\u003e\n          \u003ctd\u003e000000\u003c/td\u003e\n          \u003ctd\u003e64 bytes of data\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch1 id=\"cache-lookup-process\"\u003eCache Lookup Process\u003c/h1\u003e\n\u003cp\u003eWhen the CPU sends a memory request:\u003c/p\u003e","title":"Cache Structure \u0026 Strategies"},{"content":"Memory Hierarchy Modern computer systems use a layered memory model to balance speed, capacity, and cost. This structure is known as the memory hierarchy.\nLevel Name Speed Cost Capacity L0 Registers Fastest Highest Smallest L1 L1 Cache Very Fast Very High Tiny L2 L2 Cache Fast High Small L3 L3 Cache Medium Moderate Medium L4 Main Memory (RAM) Slower Lower Large L5 SSD/Disk Much Slower Cheapest Very Large As we go down the hierarchy:\nSpeed decreases Capacity increases Cost per bit decreases Virtual Memory Virtual memory creates the illusion of a large contiguous memory space. It maps program memory to physical memory and uses the disk as a backup when RAM is full.\nThis allows programs to run even when they need more memory than is physically available.\nPrinciple of Locality To make memory access faster, hardware leverages temporal and spatial locality.\nTemporal Locality (Locality in Time) If a memory location is accessed once, it will likely be accessed again soon.\nint x = 42; for (int i = 0; i \u0026lt; 1000000; i++) { sum += x; // frequent access to the same variable } Since x is accessed repeatedly, it stays in the cache, making access fast.\nSpatial Locality (Locality in Space) If a memory location is accessed, nearby memory locations will likely be accessed soon.\nint arr[10000]; for (int i = 0; i \u0026lt; 1000; i++) { arr[i] = i * 2; } Arrays are stored contiguously. When the CPU accesses arr[i], it also loads arr[i+1], arr[i+2], etc., into the cache.\nA Bad Locality Example int arr[1000000]; for (int i = 0; i \u0026lt; 1000000; i += 1000) { arr[i] = i; // large stride } Every access skips 1000 elements. Cache misses increase. Performance drops. Cache Levels and Naming We often abbreviate cache names like this:\nL1$ – Level 1 Cache L2$ – Level 2 Cache L3$ – Level 3 Cache D$ – Data Cache I$ – Instruction Cache These labels are common in textbooks and lectures.\nManaging the Hierarchy Transition Managed By Registers ↔ Memory Compiler / Assembly programmer Cache ↔ Main Memory Cache controller (hardware) Main Memory ↔ Disk (VM) Operating system Final Goal Use small, fast caches to simulate large, fast memory.\nMemory hierarchy and locality work together to let the CPU access data quickly without knowing it\u0026rsquo;s stored in slow memory layers.\n\u0026#34;Temporal likes to reread, Spatial loves neighbors; Cache\u0026#39;s favorite kids: repetitive and cohesive.\u0026#34; ","permalink":"https://dev-helia.github.io/Tech-Blog-Website/posts/cs/principles_of_computer_composition/10_memory/","summary":"\u003ch1 id=\"memory-hierarchy\"\u003eMemory Hierarchy\u003c/h1\u003e\n\u003cp\u003eModern computer systems use a layered memory model to balance \u003cstrong\u003espeed\u003c/strong\u003e, \u003cstrong\u003ecapacity\u003c/strong\u003e, and \u003cstrong\u003ecost\u003c/strong\u003e. This structure is known as the \u003cstrong\u003ememory hierarchy\u003c/strong\u003e.\u003c/p\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eLevel\u003c/th\u003e\n          \u003cth\u003eName\u003c/th\u003e\n          \u003cth\u003eSpeed\u003c/th\u003e\n          \u003cth\u003eCost\u003c/th\u003e\n          \u003cth\u003eCapacity\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eL0\u003c/td\u003e\n          \u003ctd\u003eRegisters\u003c/td\u003e\n          \u003ctd\u003eFastest\u003c/td\u003e\n          \u003ctd\u003eHighest\u003c/td\u003e\n          \u003ctd\u003eSmallest\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eL1\u003c/td\u003e\n          \u003ctd\u003eL1 Cache\u003c/td\u003e\n          \u003ctd\u003eVery Fast\u003c/td\u003e\n          \u003ctd\u003eVery High\u003c/td\u003e\n          \u003ctd\u003eTiny\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eL2\u003c/td\u003e\n          \u003ctd\u003eL2 Cache\u003c/td\u003e\n          \u003ctd\u003eFast\u003c/td\u003e\n          \u003ctd\u003eHigh\u003c/td\u003e\n          \u003ctd\u003eSmall\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eL3\u003c/td\u003e\n          \u003ctd\u003eL3 Cache\u003c/td\u003e\n          \u003ctd\u003eMedium\u003c/td\u003e\n          \u003ctd\u003eModerate\u003c/td\u003e\n          \u003ctd\u003eMedium\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eL4\u003c/td\u003e\n          \u003ctd\u003eMain Memory (RAM)\u003c/td\u003e\n          \u003ctd\u003eSlower\u003c/td\u003e\n          \u003ctd\u003eLower\u003c/td\u003e\n          \u003ctd\u003eLarge\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eL5\u003c/td\u003e\n          \u003ctd\u003eSSD/Disk\u003c/td\u003e\n          \u003ctd\u003eMuch Slower\u003c/td\u003e\n          \u003ctd\u003eCheapest\u003c/td\u003e\n          \u003ctd\u003eVery Large\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003eAs we go down the hierarchy:\u003c/p\u003e","title":"Memory"},{"content":"What is a pipeline? Pipelining refers to processing different parts of multiple instructions simultaneously to improve CPU throughput.\nFive-stage pipeline structure (RISC-V) IF ➝ ID ➝ EX ➝ MEM ➝ WB Graph:\nFrom cs61c This diagram shows the five classic stages in a RISC-V pipelined processor:\nInstruction Fetch (IF) – Fetch instruction from memory. Instruction Decode / Register Read (ID) – Decode the instruction and read source registers. Execute / Address Calculation (EX) – Perform ALU operations or calculate addresses. Memory Access (MEM) – Read/write data from/to memory. Write Back (WB) – Write result back to register file. Multicycle → Pipelining Five Classic Pipeline Stages in RISC-V Stage Name Description IF Instruction Fetch Fetch instruction from memory ID Instruction Decode / Register Read Decode instruction and read registers EX Execute / Address Calculation Perform ALU operations or address compute MEM Memory Access Access memory (read/write) WB Write Back Write result back to registers How do multiple instructions run in parallel? Each instruction enters the pipeline and advances one stage per cycle.\nCycle IF ID EX MEM WB Cycle 1 A Cycle 2 B A Cycle 3 C B A Cycle 4 D C B A Cycle 5 E D C B A Pipeline Registers Register Role IF/ID Stores the instruction address and content from the IF stage ID/EX Stores decoded register values, immediate values, control signals EX/MEM Stores ALU results, target addresses, data to be written MEM/WB Stores memory output or ALU result, ready to be written back Data Hazard A data hazard occurs when an instruction depends on the result of a previous instruction that hasn’t completed its write-back yet.\nSolutions: Forwarding: Fetch the result early from the EX/MEM or MEM/WB pipeline stage Stall: Pause the instruction for one or two cycles until the data is ready Comparison of Solutions: Method Mechanism Example Cycles Notes Stall Pause the pipeline until data is ready add x1, x2, x3sub x4, x1, x5 +1 ~ 2 Simple but slows performance Forwarding Forward results from EX/MEM or MEM/WB stage Same as above 0 High performance, no stall Example: add x1, x2, x3 // result written to x1 sub x4, x1, x5 // needs x1 → Data hazard! With Stall: sub must wait until add finishes WB stage With Forwarding: sub can grab x1 early from EX/MEM → No stall! Timeline when Forwarding works Cycle | add x1,x2,x3 | sub x4,x1,x5 | nop | ------|--------------|--------------|-----| C1 | IF | | | C2 | ID | IF | | C3 | EX | ID | | C4 | MEM | EX [Forward] | | C5 | WB | MEM | | C6 | | WB | | At Cycle 4:\nadd is in MEM stage, x1 result stored in EX/MEM sub is in EX stage, needs x1 Forwarding module detects it, grabs x1 from EX/MEM What if there\u0026rsquo;s no Forwarding? Cycle | add x1,x2,x3 | nop | sub x4,x1,x5 | ------|--------------|-----|--------------| C1 | IF | | | C2 | ID | IF | | C3 | EX | ID | | C4 | MEM | NOP | IF | C5 | WB | | ID | C6 | | | EX | You’ll have to insert a nop to stall the sub instruction!\nControl Hazard A control hazard occurs when a branch or jump instruction (e.g., beq) is encountered, and the next instruction to execute is uncertain.\nSolutions: Branch prediction: Predict the next instruction path Insert NOP (bubble): Stall until the branch decision is known What is a Control Hazard? When we encounter conditional branch instructions (e.g., beq, bne, jalr):\nWe don’t know which instruction to execute next — it depends on whether the branch is taken.\nThe condition is only known in the EX stage,\nBut we’ve already fetched the next instruction in the IF stage.\n→ This may lead to executing the wrong path.\nThe Most Conservative Fix: Insert a Bubble Example: beq x1, x2, label // branch instruction add x3, x4, x5 // possibly wrong instruction If the beq is taken, the add is incorrect and must be flushed.\nSo we insert a NOP (bubble) to wait for the result of beq.\nCycle | beq | add ------|-----------|----------- C1 | IF | C2 | ID | IF C3 | EX | ID (wait) C4 | MEM | EX C5 | WB | MEM Is Bubble Too Conservative? Use Branch Prediction What is Branch Prediction? Before knowing whether to take the branch, the CPU guesses:\nPredict taken → fetch the target instruction Predict not taken → continue sequentially If the guess is correct, pipelining continues\nIf the guess is wrong, the pipeline is flushed and restarted\nStatic Branch Prediction (Simplest) Rule Meaning Predict Not Taken Assume all branches are not taken Predict Taken Assume all branches are taken Advantage: Simple logic Disadvantage: Easy to mispredict Dynamic Branch Prediction Let the CPU learn from past branch behavior and make smarter predictions.\n1-bit Predictor Each branch instruction maintains a 1-bit flag:\n0 means \u0026ldquo;not taken last time\u0026rdquo;, so predict not taken 1 means \u0026ldquo;taken last time\u0026rdquo;, so predict taken Actual Outcome New Predictor State Taken vs Not Taken Change to 1 Not Taken vs Taken Change to 0 Disadvantage: Flips too often in alternating branch cases, like at loop boundaries.\n2-bit Predictor (Common and Effective) Improved predictor with 2-bit state machine:\nState Meaning Transition Rule 00 Strongly Not Taken Stay if correct, else → 01 01 Weakly Not Taken Wrong once → 10 10 Weakly Taken Wrong once → 01 11 Strongly Taken Stay if correct, else → 10 Overall structure: A branch predictor with memory that reduces fluctuation.\nFull Logic Overview (Text Version) IF: Fetch instruction (with prediction) → ID: Decode → EX: Evaluate branch condition ↑ If prediction is wrong → flush following stages Structural Hazard A structural hazard occurs when multiple pipeline stages compete for the same hardware resource (e.g., trying to access memory simultaneously).\nSolutions: Split instruction memory and data memory (Harvard Architecture) Add more hardware resources (more costly) Suppose we only have one memory block, which is responsible for both:\nInstruction Memory (fetch instructions) Data Memory (load/store data) Then we run these two instructions:\nlw x1, 0(x2) // Requires access to data memory (MEM stage) add x3, x4, x5 // Normal execution (instruction fetched in IF stage) In a certain cycle:\nlw is in MEM stage accessing data memory add is in IF stage fetching instruction → But instruction memory and data memory are the same module!\nResult: These two stages compete for memory, causing a structural hazard.\nComparison of Solutions Method Principle Example Pros and Cons Add hardware resources Separate instruction and data memory (Harvard) Two separate memories High efficiency, but costly Insert bubble Delay the conflicting stage nop or stall Simple but hurts performance Pipeline scheduling Avoid simultaneous access Smart instruction reordering Complex but effective Memory Stall A memory stall is one of the most common performance bottlenecks in pipelining.\nWhat is a Memory Stall? A memory stall occurs when the CPU or pipeline is forced to wait for a memory access to complete. During this wait, the CPU becomes stalled and stops progressing.\nIn Other Words: When you are executing a pipelined program, and the five stages are flowing smoothly\u0026hellip;\nSuddenly, at the Memory (MEM) stage, one instruction says:\n\u0026ldquo;I haven\u0026rsquo;t received the data yet\u0026hellip; You all go ahead without me!\u0026rdquo;\nThis causes the entire pipeline to stall, just like lag in a video game.\nCommon Scenarios Causing Memory Stall Scenario Description Example Load/Store instructions access memory Load or store takes too long lw, sw with slow memory Cache Miss Data not in cache, must fetch from main memory Cache miss ➝ DRAM access (100+ cycles) Multiple stages accessing memory Not enough ports for concurrent memory access IF and MEM both accessing instruction memory A Very Simple Example Suppose we execute:\nlw t0, 0(t1) And the address pointed to by t1 happens to be in memory, but causes a cache miss\u0026hellip;\nThen we must fetch from DRAM, which may take hundreds of cycles.\nThe next pipeline instruction cannot proceed, because t0 is not ready yet.\nSo the pipeline becomes stalled, waiting for memory response.\nHow to Reduce Memory Stalls Method Principle Increase Cache Hit Rate Leverage locality, preload data Add Write Buffer Allow non-blocking writes Optimize Memory Hierarchy Use L1/L2/L3 cache to reduce DRAM dependency Pipeline Forwarding + Stall Slot Avoid data-related hazards ","permalink":"https://dev-helia.github.io/Tech-Blog-Website/posts/cs/principles_of_computer_composition/09_pipeline/","summary":"\u003ch2 id=\"what-is-a-pipeline\"\u003eWhat is a pipeline?\u003c/h2\u003e\n\u003cp\u003ePipelining refers to processing different parts of multiple instructions simultaneously to improve CPU \u003cstrong\u003ethroughput\u003c/strong\u003e.\u003c/p\u003e\n\u003ch2 id=\"five-stage-pipeline-structure-risc-v\"\u003eFive-stage pipeline structure (RISC-V)\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eIF ➝ ID ➝ EX ➝ MEM ➝ WB\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg alt=\"alt text\" loading=\"lazy\" src=\"/Tech-Blog-Website/posts/cs/principles_of_computer_composition/09_pipeline/image.png\"\u003e\n\u003cimg alt=\"alt text\" loading=\"lazy\" src=\"/Tech-Blog-Website/posts/cs/principles_of_computer_composition/09_pipeline/image-1.png\"\u003e\n\u003cstrong\u003eGraph:\u003c/strong\u003e\u003cbr\u003e\n\u003cem\u003eFrom cs61c\u003c/em\u003e\nThis diagram shows the five classic stages in a RISC-V pipelined processor:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003eInstruction Fetch (IF)\u003c/strong\u003e – Fetch instruction from memory.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eInstruction Decode / Register Read (ID)\u003c/strong\u003e – Decode the instruction and read source registers.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eExecute / Address Calculation (EX)\u003c/strong\u003e – Perform ALU operations or calculate addresses.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eMemory Access (MEM)\u003c/strong\u003e – Read/write data from/to memory.\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eWrite Back (WB)\u003c/strong\u003e – Write result back to register file.\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch2 id=\"multicycle--pipelining\"\u003eMulticycle → Pipelining\u003c/h2\u003e\n\u003ch3 id=\"five-classic-pipeline-stages-in-risc-v\"\u003eFive Classic Pipeline Stages in RISC-V\u003c/h3\u003e\n\u003ctable\u003e\n  \u003cthead\u003e\n      \u003ctr\u003e\n          \u003cth\u003eStage\u003c/th\u003e\n          \u003cth\u003eName\u003c/th\u003e\n          \u003cth\u003eDescription\u003c/th\u003e\n      \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eIF\u003c/td\u003e\n          \u003ctd\u003eInstruction Fetch\u003c/td\u003e\n          \u003ctd\u003eFetch instruction from memory\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eID\u003c/td\u003e\n          \u003ctd\u003eInstruction Decode / Register Read\u003c/td\u003e\n          \u003ctd\u003eDecode instruction and read registers\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eEX\u003c/td\u003e\n          \u003ctd\u003eExecute / Address Calculation\u003c/td\u003e\n          \u003ctd\u003ePerform ALU operations or address compute\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eMEM\u003c/td\u003e\n          \u003ctd\u003eMemory Access\u003c/td\u003e\n          \u003ctd\u003eAccess memory (read/write)\u003c/td\u003e\n      \u003c/tr\u003e\n      \u003ctr\u003e\n          \u003ctd\u003eWB\u003c/td\u003e\n          \u003ctd\u003eWrite Back\u003c/td\u003e\n          \u003ctd\u003eWrite result back to registers\u003c/td\u003e\n      \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003chr\u003e\n\u003ch3 id=\"how-do-multiple-instructions-run-in-parallel\"\u003eHow do multiple instructions run in parallel?\u003c/h3\u003e\n\u003cp\u003eEach instruction enters the pipeline and advances one stage per cycle.\u003c/p\u003e","title":"Pipeline"},{"content":"Why I Decided to Build a Full-Stack Project Honestly, the motivation wasn\u0026rsquo;t pure tech passion — it was anxiety from comparison. I saw classmates with polished full-stack projects, while I had only built algorithm demos or component pages.\nSo I forced myself to make something that looks like a full-stack product.\nOn the first day I broke down:\nWhat is JSX? What’s the deal between Vite and React? Why am I getting @babel/preset-react errors in testing? What\u0026rsquo;s the difference between Supabase and Node.js?\nI knew nothing — but I still started.\nThat’s why I decided to document every clueless moment. Even if it’s not useful to others, it’s proof that: I really walked this path.\nWhy This Project? While working on another project (Herthspace), I came across several prompt-sharing sites like PromptHero and Prompt Playground.\nIt hit me:\n“If they can build such fun AI content platforms\u0026hellip; why can’t I make one too?”\nBut what truly pushed me was seeing friends with end-to-end projects — frontend + backend + login + database — all fully connected. I wanted one too. I was jealous. 👀\nInstead of copying, I started comparing what those sites actually do:\nSite What inspired me PromptHero Prompt categorization + gallery → I want \u0026ldquo;Like\u0026rdquo; and \u0026ldquo;Save\u0026rdquo; PromptLand Prompt testing + model preview → I want \u0026ldquo;built-in model test\u0026rdquo; Pinterest Card-style UI → I want an aesthetic, fun, social-ish experience A lot of prompt sites look good, but feel passive. I wanted a site people actually want to save prompts from.\nSo Promptllery was born:\nA fun little prompt playground where:\nGood prompts can be saved Liked ones can be liked and shared Users can test prompts inline No GPT-4? Use share links and QR so no tokens are wasted! Thinking Backwards from Features to Stack At this point, I worked backwards to choose my stack:\nI want gallery-style layout → React + Tailwind I want to store prompts → Supabase I want interactivity (like/save) → useState/useEffect + Supabase I want prompt testing → OpenAI API (GPT-3.5 only) I want to save token costs → shareable links Eventually Promptllery became my playground — not to show off tech skills, but to practice product thinking.\nWhy This Tech Stack? I didn’t choose the stack intentionally — I just followed what others were using.\nBut I slowly realized: this is the perfect beginner-friendly \u0026ldquo;frontend-led\u0026rdquo; stack.\nTech What it is Why it fits Promptllery React JS library for UI Each prompt is a component; like/save = reactive state JSX HTML inside JS Makes component building fast \u0026amp; intuitive Tailwind CSS Utility-first CSS Perfect for rapid design and layout Vite Build tool Super fast dev server, built for modern frameworks Supabase Backend-as-a-service No backend code needed, but still have DB \u0026amp; Auth Jest + RTL Testing framework + helpers For ensuring component logic works as expected Vercel One-click deployment Perfect for student / solo projects Why not MERN? Because I want to focus on product logic and user experience, not API/infra.\nStack MERN (Mongo + Express) My stack (React + Supabase) Backend Dev You write everything Built-in API, no backend needed Database NoSQL (MongoDB) SQL (PostgreSQL, stable) Who it\u0026rsquo;s for Backend-focused devs MVP builders, frontend-heavy people What I Actually Built I started from zero — literally.\nModule Description Implementation Details Prompt Card Gallery List prompts with titles, tags, author PromptCard component + Tailwind layout Upload Form Create new prompts with title, desc, tags Controlled form → onSubmit sends to Supabase Like + Favorite Users can react to prompts Toggle useState + update Supabase row Search \u0026amp; Filter By keyword or tag Input + .filter() over prompt list Test Prompt Inline Run prompt with GPT Call OpenAI API and display result in frontend Share Tools Copy prompt, share QR navigator.clipboard.writeText, QR from lib Supabase Management Store/query prompt info, manage likes supabase.from(...).insert() or .update() Lessons from the Struggle I thought this was just a “temporary practice project”,\nBut it became proof of something deeper:\nThis was the first product I really built from scratch.\nProblem The Struggle How I Solved It Jest not reading JSX “Unexpected token” with \u0026lt;App /\u0026gt; Installed @babel/preset-react + created .babel.config.js Node version mismatch Packages required Node 18+ Used --force first, switched later Tailwind too verbose className chains were overwhelming Started using clsx() and layout planning Upload form bugged Wrong data or no state update Used controlled inputs + useEffect properly Vercel config confusion Didn’t know about Git linking or builds Learned .env, Git setup, and build settings Final Takeaways Before this project, I only admired what others had built.\nNow I know:\nPrompt sites can be built solo Supabase is beginner-friendly Testing is annoying but necessary Debugging is a superpower There’s a huge gap between “using” and “understanding” I can decide what a product looks like — not just follow others This isn’t a show-off piece. It’s my first real tech journey from: idea → research → design → build → debug → write → deploy.\nAnd I’ll never forget this feeling.\nDemo + User Flow I deployed it online (via Vercel)\nand recorded a short video to walk through the full user experience — from uploading prompts to sharing them.\nWhat\u0026rsquo;s Next? From solo to social\nAdd user accounts, profiles, and public prompt collections.\nLet users follow others, comment, and build a prompt culture.\nFrom gallery to ecosystem\nAdd categories, remixable prompt templates, and custom tags.\nBuild a real prompt library, not just a showcase.\nFrom testing to co-creation\nLet users build prompts together:\none writes the setup, another refines the wording, a third gives sample outputs.\nFrom passive use to community-driven feedback\nWeekly prompt picks, upvoting systems, feedback for creators.\nTurn prompts into a living dialogue, not static text.\nFrom student project to long-term playground\nEventually, maybe even invite contributors.\nOpen source parts, build an API, explore monetization for top creators?\nPromptllery is not just a website — it’s my first step into building something others might actually use.\nHope it becomes something meaningful and fun, even if it started out of jealousy.\n","permalink":"https://dev-helia.github.io/Tech-Blog-Website/posts/dev/promptllery/","summary":"\u003ch2 id=\"why-i-decided-to-build-a-full-stack-project\"\u003eWhy I Decided to Build a Full-Stack Project\u003c/h2\u003e\n\u003cp\u003eHonestly, the motivation wasn\u0026rsquo;t pure tech passion — it was anxiety from comparison. I saw classmates with polished full-stack projects, while I had only built algorithm demos or component pages.\u003c/p\u003e\n\u003cp\u003eSo I forced myself to make something that \u003cem\u003elooks\u003c/em\u003e like a full-stack product.\u003c/p\u003e\n\u003cp\u003eOn the first day I broke down:\u003cbr\u003e\nWhat is JSX? What’s the deal between Vite and React? Why am I getting \u003ccode\u003e@babel/preset-react\u003c/code\u003e errors in testing? What\u0026rsquo;s the difference between Supabase and Node.js?\u003c/p\u003e","title":"My First Full-Stack Project — Promptllery"}]